import torch
import torch.nn as nn
import torch.nn.functional as F
from time_embedding import get_timestep_embedding

class BasicResidualBlock(nn.Module):
    def __init__(self, in_ch, out_ch, time_emb_dim):
        super().__init__()
        self.norm1 = nn.GroupNorm(8, in_ch)
        self.act1 = nn.SiLU()
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)

        self.norm2 = nn.GroupNorm(8, out_ch)
        self.act2 = nn.SiLU()
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)

        self.time_proj = nn.Linear(time_emb_dim, out_ch)
        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()

    def forward(self, x, t_emb):
        h = self.conv1(self.act1(self.norm1(x)))
        h = h + self.time_proj(t_emb)[:, :, None, None]
        h = self.conv2(self.act2(self.norm2(h)))
        return h + self.skip(x)

class DownBlock(nn.Module):
    def __init__(self, in_ch, out_ch, time_emb_dim):
        super().__init__()
        self.res = BasicResidualBlock(in_ch, out_ch, time_emb_dim)
        self.pool = nn.MaxPool2d(2)

    def forward(self, x, t_emb):
        x = self.res(x, t_emb)
        return self.pool(x), x  # è¾“å‡º x å’Œ skip è¿æ¥

# ğŸ”¹ ä¸Šé‡‡æ ·æ¨¡å—
class UpBlock(nn.Module):
    def __init__(self, in_ch, out_ch, time_emb_dim):
        super().__init__()
        self.res = BasicResidualBlock(in_ch + out_ch, out_ch, time_emb_dim)

    def forward(self, x, skip, t_emb):
        x = F.interpolate(x, scale_factor=2, mode="nearest")
        x = torch.cat([x, skip], dim=1)
        return self.res(x, t_emb)


class UNet(nn.Module):
    def __init__(self, in_channels=3, cond_channels=3, base_channels=64, time_emb_dim=128):
        super().__init__()
        # æ—¶é—´æ­¥ç¼–ç å™¨ï¼ˆå¯å­¦ä¹ éƒ¨åˆ†ï¼‰
        self.time_mlp = nn.Sequential(
            nn.Linear(time_emb_dim, time_emb_dim * 4),
            nn.SiLU(),
            nn.Linear(time_emb_dim * 4, time_emb_dim)
        )

        self.input_conv = nn.Conv2d(in_channels + cond_channels, base_channels, 3, padding=1)

        # ä¸‹é‡‡æ ·è·¯å¾„
        self.down1 = DownBlock(base_channels, base_channels * 2, time_emb_dim)
        self.down2 = DownBlock(base_channels * 2, base_channels * 4, time_emb_dim)

        # ä¸­é—´
        self.middle = BasicResidualBlock(base_channels * 4, base_channels * 4, time_emb_dim)

        # ä¸Šé‡‡æ ·è·¯å¾„
        self.up2 = UpBlock(base_channels * 4, base_channels * 2, time_emb_dim)
        self.up1 = UpBlock(base_channels * 2, base_channels, time_emb_dim)

        # è¾“å‡ºå±‚
        self.output_conv = nn.Conv2d(base_channels, in_channels, 1)
        self.time_emb_dim = time_emb_dim

    def forward(self, x_t, t, cond):
        """
        x_t: å½“å‰å«å™ªå›¾åƒ [B, 3, H, W]
        t: å½“å‰æ—¶é—´æ­¥æ•´æ•° [B]
        cond: æ¡ä»¶å›¾åƒ [B, 3, H, W]ï¼ˆå¦‚ Ki67ï¼‰
        """
        x = torch.cat([x_t, cond], dim=1)  # æ‹¼æ¥æ¡ä»¶å›¾åƒ
        t_emb = get_timestep_embedding(t, self.time_emb_dim)
        t_emb = self.time_mlp(t_emb)  # å˜æ¢ååµŒå…¥ [B, time_emb_dim]

        x = self.input_conv(x)
        x, skip1 = self.down1(x, t_emb)
        x, skip2 = self.down2(x, t_emb)

        x = self.middle(x, t_emb)

        x = self.up2(x, skip2, t_emb)
        x = self.up1(x, skip1, t_emb)

        return self.output_conv(x)  # è¾“å‡ºé¢„æµ‹å™ªå£° ÎµÌ‚
